#!/usr/bin/env python3
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px

# Mapowanie etykiet numerycznych na czytelne nazwy klas
label_map = {0: 'dropout', 1: 'graduate', 2: 'enrolled'}

# ----------------------------
# FUNKCJE POMOCNICZE
# ----------------------------

def softmax(Z): # zamiast sigmoidu
    """
    Softmax: zamienia surowe logity Z (NxK) na prawdopodobie≈Ñstwa (sumujƒÖ siƒô do 1 w ka≈ºdym wierszu).
    Stabilizujemy obliczenia przez odjƒôcie maksymalnej warto≈õci w ka≈ºdym wierszu.
    """
    Z_exp = np.exp(Z - Z.max(axis=1, keepdims=True))
    return Z_exp / Z_exp.sum(axis=1, keepdims=True)

def one_hot(y, K):
    """
    Zamienia wektor etykiet y (N,) na macierz one-hot (NxK).
    Dziƒôki temu mo≈ºemy u≈ºyƒá cross-entropii jako funkcji kosztu.
    """
    N = y.shape[0]
    Y = np.zeros((N, K))
    # dla ka≈ºdej pr√≥bki ustawiamy 1 w kolumnie odpowiadajƒÖcej jej etykiecie
    Y[np.arange(N), y] = 1
    return Y

def compute_loss(P, Y):
    """
    Oblicza stratƒô cross-entropy:
      -sum(Y * log(P)) / N
    Dodajemy malutkƒÖ eps (1e-15), by uniknƒÖƒá log(0).
    """
    return -np.mean(np.sum(Y * np.log(P + 1e-15), axis=1))

def train_logistic_gd_with_history(X, y, K, lr=0.05, epochs=200, batch_size=128):
    """
    Trenuje regresjƒô logistycznƒÖ metodƒÖ mini-batch gradient descent.
    Zwraca finalne wagi W oraz historiƒô strat (po ka≈ºdej epoce).
    """
    N, D = X.shape
    # inicjalizacja wag: D cech √ó K klas
    W = np.zeros((D, K))
    # przygotowanie one-hot dla etykiet
    Y = one_hot(y, K)

    history_loss = []
    # history_W mo≈ºesz odkomentowaƒá, je≈õli chcesz ≈õledziƒá same wagi
    # history_W = []

    for epoch in range(1, epochs+1):
        # losowo permutujemy pr√≥bki, by batche by≈Çy r√≥≈ºne co epokƒô
        perm = np.random.permutation(N)
        X_shuff, Y_shuff = X[perm], Y[perm]

        # dzielimy na mini-batche
        for start in range(0, N, batch_size):
            Xb = X_shuff[start:start+batch_size]
            Yb = Y_shuff[start:start+batch_size]

            # krok forward: logity i softmax
            Z  = Xb @ W
            P  = softmax(Z)

            # gradient cross-entropy: X^T (P - Y) / batch_size
            grad = Xb.T @ (P - Yb) / Xb.shape[0]

            # update wag
            W -= lr * grad

        # po ka≈ºdej epoce liczymy stratƒô na ca≈Çym zbiorze treningowym
        P_full = softmax(X @ W)
        loss = compute_loss(P_full, Y)
        history_loss.append(loss)
        # history_W.append(W.copy())

        # co 10 epok (i za pierwszym razem) drukujemy postƒôp
        if epoch % 10 == 0 or epoch == 1:
            print(f"Epoch {epoch:3d}/{epochs} ‚Äî loss: {loss:.4f}")

    return W, history_loss

def plot_loss_curve(loss_history, out_dir):
    """
    Rysuje i zapisuje krzywƒÖ strat (loss vs epochs).
    """
    plt.figure()
    plt.plot(range(1, len(loss_history)+1), loss_history, marker='o')
    plt.title('Cross-Entropy Loss vs Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, 'loss_curve.png'))
    plt.close()

def plot_probs(probs, out_dir):
    """
    Dla ka≈ºdej klasy rysuje:
     - wykres liniowy prawdopodobie≈Ñstwa w kolejnych pr√≥bkach
     - histogram rozk≈Çadu tych prawdopodobie≈Ñstw
    """
    # nazwy kolumn wed≈Çug label_map
    cols = [label_map[i] for i in range(probs.shape[1])]
    dfp = pd.DataFrame(probs, columns=cols)

    # 1) wykresy liniowe
    for cls in cols:
        plt.figure()
        plt.plot(dfp.index, dfp[cls], lw=1)
        plt.title(f"Prawdopodobie≈Ñstwo klasy ‚Äú{cls}‚Äù")
        plt.xlabel("Indeks pr√≥bki")
        plt.ylabel("Prawdopodobie≈Ñstwo")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{cls}_line.png"))
        plt.close()

    # 2) histogramy
    for cls in cols:
        plt.figure()
        plt.hist(dfp[cls], bins=20)
        plt.title(f"Rozk≈Çad prawdopodobie≈Ñstwa klasy ‚Äú{cls}‚Äù")
        plt.xlabel("Prawdopodobie≈Ñstwo")
        plt.ylabel("Liczba pr√≥bek")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{cls}_hist.png"))
        plt.close()

# ----------------------------
# G≈Å√ìWNY BLOK
# ----------------------------

def main():
    # katalog na wszystkie wyniki i wykresy
    out_dir = "logreg_results"
    os.makedirs(out_dir, exist_ok=True)

    # 1. Wczytanie danych z CSV
    #    - Pandas do I/O, p√≥≈∫niej tylko NumPy na macierzach
    # df = pd.read_csv("data/formatted_dataset.csv")
    df = pd.read_csv("data/dane_bez1.csv")


    counts = df["Target"].value_counts().sort_index()  # liczba pr√≥bek w ka≈ºdej klasie
    props  = df["Target"].value_counts(normalize=True).sort_index() * 100  # procentowo

    print("Rozk≈Çad klas w Target (liczba pr√≥bek):")
    print(counts)
    print("\nRozk≈Çad klas w Target (procentowo):")
    print(props.round(2).astype(str) + " %")

    # fig = px.histogram(df, x="Target", nbins=30, marginal="box")
    # fig.show(renderer="browser")

    y = df["Target"].to_numpy()
    X = df.drop(columns=["Target"]).to_numpy()

    # 2. Podzia≈Ç na zbi√≥r treningowy i testowy (80/20)
    N = X.shape[0] # to liczba wierszy (pr√≥bek) w macierzy
    split = int(0.8 * N)
    # Pierwsze split pr√≥bek to trening, pozosta≈Çe to test
    X_train = X[:split]    # wiersze od 0 do split-1
    X_test  = X[split:]    # wiersze od split do N-1

    y_train = y[:split]    # odpowiadajƒÖce etykiety
    y_test  = y[split:]

    # 3. Standaryzacja cech: (x - mean) / std na podstawie X_train
    mean = X_train.mean(axis=0)
    std  = X_train.std(axis=0)
    std[std == 0] = 1
    X_train = (X_train - mean) / std
    X_test  = (X_test  - mean) / std

    # 4. Dodanie bias (kolumna jedynek) ‚Äì pozwala modelowi mieƒá wyraz wolny w r√≥wnaniu
    X_train = np.hstack([np.ones((X_train.shape[0],1)), X_train])
    X_test  = np.hstack([np.ones((X_test.shape[0],1)),  X_test])
    # Pozwala modelowi przesunƒÖƒá funkcjƒô decyzyjnƒÖ lub liniƒô w g√≥rƒô/d√≥≈Ç,
    # Bez biasu funkcja zawsze przechodzi≈Çaby przez poczƒÖtek uk≈Çadu, co ogranicza jej mo≈ºliwo≈õci dopasowania.

    # 5. Trening modelu
    K = len(label_map)  # liczbƒô klas bierzemy z d≈Çugo≈õci mapy
    W, loss_history = train_logistic_gd_with_history(
        X_train,    # macierz cech treningowych (N_train √ó (D+1)), zawiera bias
        y_train,    # wektor etykiet treningowych (N_train,)
        K,          # liczba klas
        lr=0.05,    # wsp√≥≈Çczynnik uczenia (learning rate)
        epochs=200, # liczba pe≈Çnych przej≈õƒá przez dane (epok)
        batch_size=128  # rozmiar mini-batch‚Äôy
    )
    
    # 6. Ewaluacja na zbiorze testowym
    P_test = softmax(X_test @ W)          # przewidywane prawdopodobie≈Ñstwa
    y_pred = np.argmax(P_test, axis=1)    # wyb√≥r klasy o najwy≈ºszym P
    acc = np.mean(y_pred == y_test)
    print(f"Accuracy na zbiorze testowym: {acc:.4f}")

    # 7. Zapis wag i wynik√≥w do plik√≥w CSV
    pd.DataFrame(W, columns=[label_map[i] for i in range(K)]) \
      .to_csv(f"{out_dir}/weights_logreg.csv", index=False)
    pd.DataFrame({"y_true": y_test, "y_pred": y_pred}) \
      .to_csv(f"{out_dir}/predictions_logreg.csv", index=False)
    pd.DataFrame(P_test, columns=[label_map[i] for i in range(K)]) \
      .to_csv(f"{out_dir}/probs_logreg.csv", index=False)

    # 8. Generowanie wykres√≥w
    print("üîç Generujƒô krzywƒÖ strat...")
    plot_loss_curve(loss_history, out_dir)

    print("üîç Generujƒô wykresy prawdopodobie≈Ñstw...")
    plot_probs(P_test, out_dir)

if __name__ == "__main__":
    main()
