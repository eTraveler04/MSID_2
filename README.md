## ğŸ“˜ Tabela wynikÃ³w â€“ interpretacja metryk


| Kolumna             | Znaczenie                                                                              |
| ------------------- | -------------------------------------------------------------------------------------- |
| `Dataset`           | ZbiÃ³r danych, na ktÃ³rym liczono metryki: `train` (treningowy), `test`                  |
| `Accuracy`          | Procent wszystkich prÃ³bek, ktÃ³re zostaÅ‚y poprawnie sklasyfikowane                      |
| `Precision (macro)` | Åšrednia precyzja dla kaÅ¼dej klasy â€“ ile z przewidzianych â€takâ€ to byÅ‚o prawdziwe â€takâ€ |
| `Recall (macro)`    | Åšrednia czuÅ‚oÅ›Ä‡ â€“ ile prawdziwych â€takâ€ zostaÅ‚o poprawnie wykrytych                    |
| `F1-score (macro)`  | Åšrednia harmoniczna precyzji i czuÅ‚oÅ›ci â€“ rÃ³wnowaÅ¼y oba wskaÅºniki                      |
| `Model`             | Nazwa modelu â€“ tutaj: `Logistic Regression` (sklearn vs wÅ‚asna wersja)                 |


## Tabela wynikÃ³w dla wÅ‚asnej implementacji sklearn ( zadanie 3.0 )
![alt text](other/image.png)

## Tabela wynikÃ³w dla wÅ‚asnej implementacji regresji logistycznej ( zadanie 4.0 )
![alt text](other/image-1.png)

## ğŸ“Š PorÃ³wnanie wynikÃ³w regresji logistycznych


| Dataset   | Metryka           | Sklearn (`zadanie 3.0`) | WÅ‚asna (`zadanie 4.0`) |
| --------- | ----------------- | ----------------------- | ---------------------- |
| **train** | Accuracy          | 0.92                    | 0.92                   |
|           | Precision (macro) | 0.93                    | 0.92                   |
|           | Recall (macro)    | 0.91                    | 0.90                   |
|           | F1-score (macro)  | 0.92                    | 0.91                   |
| **test**  | Accuracy          | 0.91                    | 0.91                   |
|           | Precision (macro) | 0.92                    | 0.91                   |
|           | Recall (macro)    | 0.90                    | 0.90                   |
|           | F1-score (macro)  | 0.91                    | 0.90                   |


## Wnioski 

Obie wersje modelu dajÄ… prawie identyczne wyniki, co oznacza, Å¼e  wÅ‚asna implementacja dziaÅ‚a bardzo dobrze.

Sklearn jest minimalnie lepszy, co prawdopodobnie wynika z zastosowania:

lepszego optymalizatora 

wbudowanej regularyzacji L2

precyzyjnych tolerancji numerycznych

RÃ³Å¼nice â‰¤ 0.01

## 4.0 1a Metryki regresji:
ğŸ”¹ MSE â€“ Mean Squared Error: 23.2541
Åšrednia z kwadratÃ³w bÅ‚Ä™dÃ³w predykcji
Im niÅ¼szy, tym lepiej

WartoÅ›Ä‡ 23.25 oznacza, Å¼e Å›redni bÅ‚Ä…d podniesiony do kwadratu wynosi 23.25 latÂ²

ğŸ”¹ MAE â€“ Mean Absolute Error: 3.3868
Åšrednia z wartoÅ›ci bezwzglÄ™dnych bÅ‚Ä™dÃ³w
Oznacza, Å¼e przeciÄ™tnie model myli siÄ™ o 3.39 roku przy przewidywaniu wieku zapisu

Jednostka: lata â€” bez podnoszenia do kwadratu

ğŸ”¹ RMSE â€“ Root Mean Squared Error: 4.8223
Pierwiastek z MSE

TeÅ¼ ma jednostkÄ™ â€lataâ€
Jest bardziej â€wraÅ¼liwy na duÅ¼e bÅ‚Ä™dyâ€ niÅ¼ MAE

MÃ³wi: przeciÄ™tnie bÅ‚Ä…d to ~4.82 roku

ğŸ”¹ RÂ² â€“ wspÃ³Å‚czynnik determinacji: 0.4800
Zakres: od 0 do 1 (czasem teÅ¼ <0, jeÅ›li model kompletnie zawodzi)

0.48 oznacza, Å¼e TwÃ³j model wyjaÅ›nia 48% wariancji wieku zapisu

PozostaÅ‚e 52% to â€szumâ€ lub coÅ›, czego model nie jest w stanie przewidzieÄ‡. 

WiÄ™c model jest Å›redni. 

## 4.0 1b Przeanalizuj ograniczenia zastosowania zamkniÄ™tej formuÅ‚y. Dlaczego nie jest w praktyce wykorzysytwana?

- Koszt obliczeniowy i pamiÄ™ciowy
Aby obliczyÄ‡,MnoÅ¼enie macierzy, operacji, a odwrÃ³cenie tej macierzy to 
Gdy masz np. 10 000 cech, odwracanie macierzy wymaga setek miliardÃ³w operacji i ogromnej iloÅ›ci pamiÄ™ci RAM.

- Brak skalowalnoÅ›ci do ogromnych danych
Dla zbiorÃ³w o milionach prÃ³bek lub dziesiÄ…tkach tysiÄ™cy cech pojÄ™cie â€wszystko w pamiÄ™ciâ€ siÄ™ rozpada.

- Prawdziwe aplikacje uczÄ… modele na strumieniach danych lub w miniâ€batchach, co zamkniÄ™ta formuÅ‚a uniemoÅ¼liwia.

-  Tylko regresja liniowa
ZamkniÄ™ta formuÅ‚a zadziaÅ‚a jedynie dla regresji z MSE (bÅ‚Ä™dem kwadratowym) i modelu liniowego.

W closed-form nie masz kontroli nad uczeniem
Nie moÅ¼esz uÅ¼yÄ‡ early stopping, ani Å›ledziÄ‡ postÄ™pu jak w gradient descent

Nie zadziaÅ‚a dobrze, jeÅ›li kolumny sÄ… skorelowane
JeÅ›li w danych sÄ… kolumny, ktÃ³re sÄ… kombinacjÄ… innych (np. â€sumaâ€ dwÃ³ch kolumn), macierz moÅ¼e byÄ‡ nieodwracalna.
Wtedy np.linalg.inv(...) moÅ¼e rzuciÄ‡ bÅ‚Ä…d lub daÄ‡ niestabilne wyniki

- nie ma pÄ™tli, nie robisz tego w kaÅ¼dej epoce jak w gradient descent.

